# base data receiver config  ---------------------------------------------------------------------
collector.receiver.base.ip=0.0.0.0
collector.receiver.base.port=9994

# number of tcp worker threads
collector.receiver.base.worker.threadSize=8
# capacity of tcp worker queue
collector.receiver.base.worker.queueSize=1024
# monitoring for tcp worker
collector.receiver.base.worker.monitor=true


# stat receiver config  ---------------------------------------------------------------------
collector.receiver.stat.udp=false
collector.receiver.stat.udp.ip=0.0.0.0
collector.receiver.stat.udp.port=9995
#stat udp socket의 receiveBufferSize설정. 만약 udp 손실율이 심할 경우 아래 사항을 체크해볼것.
collector.receiver.stat.udp.receiveBufferSize=4194304

# Should keep in mind that TCP transport load balancing is per connection.(UDP transport loadbalancing is per packet)
collector.receiver.stat.tcp=true
collector.receiver.stat.tcp.ip=0.0.0.0
collector.receiver.stat.tcp.port=9995

# stat메시지를 처리할 thread 수, hbase.hTablePoolSize사이즈와 같이 조절해야 한다.
collector.receiver.stat.worker.threadSize=16
# stat메시지를 처리할 thread가 모두 동작중일때 요청을 몇개까지 큐잉할지?
collector.receiver.stat.worker.queueSize=512
# monitoring for udp stat worker
collector.receiver.stat.worker.monitor=true

# span receiver config  ---------------------------------------------------------------------
collector.receiver.span.udp=false
collector.receiver.span.udp.ip=0.0.0.0
collector.receiver.span.udp.port=9996
#span UDP socket의 receiveBufferSize설정. 만약 udp 손실율이 심할 경우 아래 사항을 체크해볼것.
collector.receiver.span.udp.receiveBufferSize=4194304

# Should keep in mind that TCP transport load balancing is per connection.(UDP transport loadbalancing is per packet)
collector.receiver.span.tcp=true
collector.receiver.span.tcp.ip=0.0.0.0
collector.receiver.span.tcp.port=9996

# span 메시지를 처리할 thread 수, hbase.hTablePoolSize사이즈와 같이 조절해야 한다.
collector.receiver.span.worker.threadSize=32
# span메시지를 처리할 thread가 모두 동작중일때 요청을 몇개까지 큐잉할지?
collector.receiver.span.worker.queueSize=1024
# monitoring for udp stat worker
collector.receiver.span.worker.monitor=true


# L4 tcp 채널 close 무시를 위한 설정.
collector.l4.ip=


#$ /sbin/sysctl -a | grep -e rmem -e wmem
#net.core.wmem_max = 524288
#net.core.rmem_max = 524288
#net.core.wmem_default = 229376
#net.core.rmem_default = 229376
#위 설정을 다음과 같이 수정합니다.
#irteamsu$ sudo sysctl -w net.core.rmem_max=4194304

# agent event worker
collector.agentEventWorker.threadSize=8
collector.agentEventWorker.queueSize=1024

# 콜렉터 JMX 관리모드 설정
collector.admin.api.jmx.active=true

# 통계값 flush 주기
statistics.flushPeriod=1000

cluster.enable=true
cluster.zookeeper.address=dev-pinpoint-local-st01.ncl.nhnsystem.com:2181
cluster.zookeeper.sessiontimeout=30000
cluster.listen.ip=
cluster.listen.port=

#cluster.enable=true
#cluster.zookeeper.address=10.25.141.69:29253
#cluster.zookeeper.sessiontimeout=10000


collector.spanEvent.sequence.limit=10000

# Flink configuration
flink.cluster.enable=false
flink.cluster.zookeeper.address=alpha.zk.pinpoint.navercorp.com
flink.cluster.zookeeper.sessiontimeout=3000

# span stat Flink configuration
span.stat.flink.cluster.enable=false
span.stat.flink.cluster.zookeeper.address=alpha.zk.pinpoint.navercorp.com
span.stat.flink.cluster.zookeeper.sessiontimeout=3000

namespaceInfo.use.defaultValue=true

collector.receiver.token.zookeeper.address=dev-pinpoint-local-st01.ncl.nhnsystem.com:2181
collector.receiver.token.zookeeper.path=/pinpoint/security/token

# gRPC
# Agent
collector.receiver.grpc.agent.enable=true
collector.receiver.grpc.agent.ip=0.0.0.0
collector.receiver.grpc.agent.port=9991
# Executor of Server
collector.receiver.grpc.agent.server.executor.thread.size=4
collector.receiver.grpc.agent.server.executor.queue.size=256
collector.receiver.grpc.agent.server.executor.monitor.enable=true
# Executor of Worker
collector.receiver.grpc.agent.worker.executor.thread.size=8
collector.receiver.grpc.agent.worker.executor.queue.size=1024
collector.receiver.grpc.agent.worker.executor.monitor.enable=true
# Server Option
collector.receiver.grpc.agent.keepalive.time.millis=300000
collector.receiver.grpc.agent.keepalive.timeout.millis=60000
collector.receiver.grpc.agent.permit.keepalive.time.millis=180000
collector.receiver.grpc.agent.connection.idle.timeout.millis=360000
collector.receiver.grpc.agent.concurrent-calls.per-connection.max=128
collector.receiver.grpc.agent.handshake.timeout.millis=120000
collector.receiver.grpc.agent.flow-control.window.size.init=1M
collector.receiver.grpc.agent.header.list.size.max=1K
collector.receiver.grpc.agent.inbound.message.size.max=4M
collector.receiver.grpc.agent.receive.buffer.size=64K

# Stat
collector.receiver.grpc.stat.enable=true
collector.receiver.grpc.stat.ip=0.0.0.0
collector.receiver.grpc.stat.port=9992
# Executor of Server
collector.receiver.grpc.stat.server.executor.thread.size=4
collector.receiver.grpc.stat.server.executor.queue.size=256
collector.receiver.grpc.stat.server.executor.monitor.enable=true
# Executor of Worker
collector.receiver.grpc.stat.worker.executor.thread.size=16
collector.receiver.grpc.stat.worker.executor.queue.size=512
collector.receiver.grpc.stat.worker.executor.monitor.enable=true
# Stream scheduler for rejected execution
collector.receiver.grpc.stat.stream.scheduler.thread.size=1
collector.receiver.grpc.stat.stream.scheduler.period.millis=1000
collector.receiver.grpc.stat.stream.call.init.request.count=16
# Server Option
collector.receiver.grpc.stat.keepalive.time.millis=300000
collector.receiver.grpc.stat.keepalive.timeout.millis=60000
collector.receiver.grpc.stat.permit.keepalive.time.millis=180000
collector.receiver.grpc.stat.connection.idle.timeout.millis=360000
collector.receiver.grpc.stat.concurrent-calls.per-connection.max=1024
collector.receiver.grpc.stat.handshake.timeout.millis=120000
collector.receiver.grpc.stat.flow-control.window.size.init=1M
collector.receiver.grpc.stat.header.list.size.max=1K
collector.receiver.grpc.stat.inbound.message.size.max=4M
collector.receiver.grpc.stat.receive.buffer.size=64K

# Span
collector.receiver.grpc.span.enable=true
collector.receiver.grpc.span.ip=0.0.0.0
collector.receiver.grpc.span.port=9993
# Executor of Server
collector.receiver.grpc.span.server.executor.thread.size=4
collector.receiver.grpc.span.server.executor.queue.size=256
collector.receiver.grpc.span.server.executor.monitor.enable=true
# Executor of Worker
collector.receiver.grpc.span.worker.executor.thread.size=32
collector.receiver.grpc.span.worker.executor.queue.size=1024
collector.receiver.grpc.span.worker.executor.monitor.enable=true
# Stream scheduler for rejected execution
collector.receiver.grpc.span.stream.scheduler.thread.size=1
collector.receiver.grpc.span.stream.scheduler.period.millis=1000
collector.receiver.grpc.span.stream.call.init.request.count=32
# Server Option
collector.receiver.grpc.span.keepalive.time.millis=300000
collector.receiver.grpc.span.keepalive.timeout.millis=60000
collector.receiver.grpc.span.permit.keepalive.time.millis=180000
collector.receiver.grpc.span.connection.idle.timeout.millis=360000
collector.receiver.grpc.span.concurrent-calls.per-connection.max=1024
collector.receiver.grpc.span.handshake.timeout.millis=120000
collector.receiver.grpc.span.flow-control.window.size.init=1M
collector.receiver.grpc.span.header.list.size.max=1K
collector.receiver.grpc.span.inbound.message.size.max=4M
collector.receiver.grpc.span.receive.buffer.size=64K

collector.receiver.channel.properties.key=licenseKey